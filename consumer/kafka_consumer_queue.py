import json
import multiprocessing
from confluent_kafka import Consumer, TopicPartition
from datetime import datetime
from pymongo import MongoClient

# MongoDB Writer í”„ë¡œì„¸ìŠ¤
def mongo_writer(write_queue: multiprocessing.Queue):
    mongo_client = MongoClient("mongodb://localhost:27017/")
    collection = mongo_client["sensor_database"]["sensor_data"]
    print("ğŸ“ MongoDB Writer í”„ë¡œì„¸ìŠ¤ ì‹œì‘ë¨")

    buffer = []

    while True:
        try:
            # ë°ì´í„°ê°€ queueì— ë“¤ì–´ì˜¬ ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¼ (timeout ê°€ëŠ¥)
            try:
                data = write_queue.get()
            except queue.Full:
                print("âš ï¸ íê°€ ê°€ë“ ì°¼ìŠµë‹ˆë‹¤. ì ì‹œ ëŒ€ê¸°í•©ë‹ˆë‹¤...", flush=True)

            # ì¢…ë£Œ ì‹ í˜¸
            if data == "STOP":
                break

            buffer.append(data)

            # ì¼ì •ëŸ‰ ìŒ“ì´ë©´ í•œ ë²ˆì— ì €ì¥
            if len(buffer) >= 1000:
                collection.insert_many(buffer)
                print(f"ğŸ“ MongoDB ì €ì¥ ì™„ë£Œ: {len(buffer)}ê±´", flush=True)
                buffer.clear()

        except Exception as e:
            print(f"ğŸš¨ MongoDB ì €ì¥ ì˜¤ë¥˜: {e}", flush=True)

    # ì¢…ë£Œ ì‹œ ë‚¨ì€ ë²„í¼ ì •ë¦¬
    if buffer:
        collection.insert_many(buffer)
    print("ğŸ§¹ MongoDB Writer ì¢…ë£Œ")

# âœ… Kafka Consumer í”„ë¡œì„¸ìŠ¤
def kafka_consumer_worker(partition_id: int, write_queue: multiprocessing.Queue):
    print(f"ğŸ›° Kafka Consumer í”„ë¡œì„¸ìŠ¤ ì‹œì‘: íŒŒí‹°ì…˜ {partition_id}")

    # Kafka Consumer ì„¤ì •
    conf = {
        "bootstrap.servers": "localhost:9092",             # Kafka ë¸Œë¡œì»¤ ì£¼ì†Œ
        "group.id": f"sensor-group-{partition_id}",        # Consumer group ID (íŒŒí‹°ì…˜ë³„ ìœ ë‹ˆí¬)
        "auto.offset.reset": "earliest",                   # ì˜¤í”„ì…‹ ì—†ì„ ê²½ìš° ê°€ì¥ ì²˜ìŒë¶€í„° ì½ê¸°
        "enable.auto.commit": False                        # ìˆ˜ë™ ì»¤ë°‹ (ì •í™•í•œ ë©”ì‹œì§€ ì²˜ë¦¬ ë³´ì¥)
    }

    consumer = Consumer(conf)
    consumer.assign([TopicPartition("sensor_data", partition_id)]) # íŒŒí‹°ì…˜ ì§€ì •í•˜ì—¬ êµ¬ë…

    try:
        while True:
            msg = consumer.poll(0.1)
            if msg is None:
                continue
            if msg.error():
                print(f"âŒ Kafka ì—ëŸ¬: {msg.error()}")
                continue

            try:
                data = json.loads(msg.value().decode("utf-8")) # Kafka ë©”ì‹œì§€ë¥¼ JSON íŒŒì‹±
                data["timestamp"] = datetime.fromisoformat(data["timestamp"]) # timestamp í•„ë“œë¥¼ ë¬¸ìì—´ì—ì„œ datetimeìœ¼ë¡œ ë³€í™˜

                write_queue.put(data)  # âœ… íì— ë°ì´í„° ë„£ê¸°
                
                consumer.commit(msg) # ë©”ì‹œì§€ ì»¤ë°‹
            except Exception as e:
                print(f"ğŸš¨ ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    finally:
        consumer.close()

# âœ… ë©”ì¸
if __name__ == "__main__":
    process_count = 10  # Kafka íŒŒí‹°ì…˜ ìˆ˜ = Consumer ìˆ˜
    write_queue = multiprocessing.Queue(maxsize=10000) # MongoDBì— ì“¸ ë°ì´í„° í (ìµœëŒ€ 10,000ê±´)

    # MongoDB ì“°ê¸° ì „ìš© í”„ë¡œì„¸ìŠ¤ ì‹œì‘
    writer_proc = multiprocessing.Process(target=mongo_writer, args=(write_queue,))
    writer_proc.start()

    # Kafka Consumer í”„ë¡œì„¸ìŠ¤ ì‹œì‘
    consumers = []
    for pid in range(process_count):
        proc = multiprocessing.Process(target=kafka_consumer_worker, args=(pid, write_queue))
        proc.start()
        consumers.append(proc)

    try:
        # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ê°€ ì¢…ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        for proc in consumers:
            proc.join()
    except KeyboardInterrupt:
        print("ğŸ›‘ ì¢…ë£Œ ì‹ í˜¸ ê°ì§€ë¨")

    # âœ… Writerì—ê²Œ ì¢…ë£Œ ì‹ í˜¸ ë³´ë‚´ê¸°
    write_queue.put("STOP")
    writer_proc.join()

    print("âœ… ì „ì²´ ì¢…ë£Œ ì™„ë£Œ")
